# -*- coding: utf-8 -*-
"""SciBERT demo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HUoQiRKgwlTfZv44ZT8vrJqtSY9B2sXE
"""



!pip install --quiet scann
!pip install --quiet datasets
import pandas as pd 
import numpy as np 

import sklearn
from sklearn.model_selection import train_test_split
import tensorflow as tf 
from tensorflow import keras 
from tensorflow.keras import layers, Input, Model
from tensorflow.keras.layers import *
import tensorflow_hub as hub 

from sklearn.metrics import confusion_matrix

import scann
from data_loader import data_loader
from negative_maker import negative_maker
from model import model
import datasets
from platform import python_version
import os

use_hub = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4") ## universal sentence encoder model
weights_path = 'drive/MyDrive/Colab Notebooks/quick_response/triplet_model_weights.h5'
saving_path = 'drive/MyDrive/Colab Notebooks/quick_response/scann_save'

train_df = data_loader('train_eli5').frame_maker()
test_df = data_loader('test_eli5').frame_maker()

train_df = negative_maker(train_df).neg_maker()

### LOSS FUNCTION
def distance_calc(y_true, y_pred):
    anchor, positive, negative = tf.split(y_pred, 3, axis=1)
    ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)
    an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)
    loss = ap_distance - an_distance
    margin = 0
    loss = tf.maximum(loss + margin, 0.0)
    return loss

triplet_model = model()
triplet_model.compile(
    optimizer = 'Adam',
    loss = distance_calc
)
y_dummy = np.ones(len(train_df)).reshape(-1,1)
triplet_model.fit([np.array(train_df['title']),
                   np.array(train_df['first_answer']),
                   np.array(train_df['neg_answer'])
                   ],
                   y_dummy,
                   epochs = 4,
                  batch_size = 64*64
                  )

triplet_model.save_weights(weights_path)
triplet_model.load_weights(weights_path)

use_emb =  triplet_model.get_layer('sentence_encoder')



q_0 = []
use_emb_test = []
for i in range(len(test_df)):
    test_answer = test_df['first_answer'][i]
    y = np.array(use_emb(([test_answer]))).reshape(1,512)
    use_emb_test.append(y)

use_emb_test = np.squeeze(np.array(use_emb_test), axis  =1)

#### vector values of the test answer embedding will be stored in the 
#### vector similarity search library scaNN

searcher = scann.scann_ops_pybind.builder(use_emb_test, 40, "dot_product").tree(
    num_leaves=2000, num_leaves_to_search=100, training_sample_size=250000).score_ah(
    2, anisotropic_quantization_threshold=0.2).reorder(100).build()

## create serialize target dir
os.makedirs(saving_path, exist_ok=True)
## serialize the searcher
searcher.serialize(saving_path)

